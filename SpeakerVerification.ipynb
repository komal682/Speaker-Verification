{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komal682/Speaker-Verification/blob/main/SpeakerVerification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-NV9inAKcn9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm  # For progress bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-J1OK7q8s2R"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEAVHeFNKI8r",
        "outputId": "492d664e-e034-4e86-98ae-89e971761026"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_path = r\"C:\\Users\\prkom\\OneDrive\\Desktop\\MTech\\1st-sem\\ML\\Project\\SpeakerVerification\"\n",
        "\n",
        "len(os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuY9xHwfZkyt"
      },
      "source": [
        "### New Load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpq0BbmEZlKX"
      },
      "outputs": [],
      "source": [
        "audio_data = {}\n",
        "\n",
        "# Traverse all directories and subdirectories\n",
        "for root, _, files in os.walk(dataset_path):\n",
        "    # Extract speaker ID from the folder structure\n",
        "    speaker_id = os.path.basename(os.path.dirname(root))\n",
        "    # Add only .wav files\n",
        "    wav_files = [os.path.join(root, file) for file in files if file.endswith('.wav')]\n",
        "\n",
        "    if wav_files:  # If there are .wav files in this folder\n",
        "        if speaker_id not in audio_data:\n",
        "            audio_data[speaker_id] = []\n",
        "        audio_data[speaker_id].extend(wav_files)  # Append files to the speaker's list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhnbj6WN9EhO"
      },
      "source": [
        "### Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJP4n2H7ZpA0"
      },
      "outputs": [],
      "source": [
        "def create_audio_pairs(audio_data):\n",
        "    pairs = []\n",
        "    speakers = list(audio_data.keys())\n",
        "\n",
        "    # Create positive pairs\n",
        "    for speaker_id, files in audio_data.items():\n",
        "        if len(files) > 1:  # Check if there are at least two files for the speaker\n",
        "            for i in range(len(files)):\n",
        "                for j in range(i + 1, len(files)):  # Pair each file with every other file in the same folder\n",
        "                    pairs.append((files[i], files[j], 1))  # Label 1 for same speaker\n",
        "        else:\n",
        "            print(f\"Speaker {speaker_id} has less than 2 files. Skipping positive pair generation.\")\n",
        "\n",
        "    # Create negative pairs\n",
        "    if len(speakers) < 2:  # Ensure there are at least 2 speakers\n",
        "        print(\"Not enough speakers to create negative pairs.\")\n",
        "        return pairs\n",
        "\n",
        "  #Selects speakers Randomly, to keep balacned dataset for 1 an 0\n",
        "\n",
        "    for _ in range(len(pairs)):  # Generate as many negative pairs as positive pairs\n",
        "        speaker1, speaker2 = random.sample(speakers, 2)  # Pick two different speakers\n",
        "        file1 = random.choice(audio_data[speaker1])\n",
        "        file2 = random.choice(audio_data[speaker2])\n",
        "        pairs.append((file1, file2, 0))  # Label 0 for different speakers\n",
        "\n",
        "    return pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfahvfgyZ_4n",
        "outputId": "caf00007-1ac8-4ded-f369-53d21c910949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total pairs created: 448960\n",
            "Sample pair: ('C:\\\\Users\\\\prkom\\\\OneDrive\\\\Desktop\\\\MTech\\\\1st-sem\\\\ML\\\\Project\\\\SpeakerVerification\\\\id10270\\\\5r0dWxy17C8\\\\00001.wav', 'C:\\\\Users\\\\prkom\\\\OneDrive\\\\Desktop\\\\MTech\\\\1st-sem\\\\ML\\\\Project\\\\SpeakerVerification\\\\id10270\\\\5r0dWxy17C8\\\\00002.wav', 1)\n"
          ]
        }
      ],
      "source": [
        "pairs = create_audio_pairs(audio_data)\n",
        "if not pairs:\n",
        "    print(\"No pairs were created. Check your data.\")\n",
        "else:\n",
        "    print(f\"Total pairs created: {len(pairs)}\")\n",
        "    print(f\"Sample pair: {pairs[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyT_iRis9nug"
      },
      "source": [
        "### Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOcmJhHeuz-w"
      },
      "outputs": [],
      "source": [
        "def extract_features(audio_path, sr=22050, wavelet_duration=1.0, feature_type='mfcc'):\n",
        "    audio, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    # Calculate wavelet size in samples\n",
        "    wavelet_size = int(sr * wavelet_duration)\n",
        "    num_wavelets = len(audio) // wavelet_size\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for i in range(num_wavelets):\n",
        "        # Slice the wavelet\n",
        "        wavelet = audio[i * wavelet_size : (i + 1) * wavelet_size]\n",
        "\n",
        "        # Extract features for the wavelet\n",
        "        if feature_type == 'mfcc':\n",
        "            wavelet_features = librosa.feature.mfcc(y=wavelet, sr=sr, n_mfcc=13)\n",
        "            # Aggregate the features, e.g., by averaging across time frames\n",
        "            wavelet_features = np.mean(wavelet_features, axis=1)\n",
        "        elif feature_type == 'spectrogram':\n",
        "            wavelet_features = np.abs(librosa.stft(wavelet))\n",
        "            wavelet_features = np.mean(wavelet_features, axis=1)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported feature type. Use 'mfcc' or 'spectrogram'.\")\n",
        "\n",
        "        features.append(wavelet_features)\n",
        "\n",
        "    # Aggregate features from all wavelets\n",
        "    aggregated_features = np.mean(features, axis=0)\n",
        "    return aggregated_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uin8F4R8diOW",
        "outputId": "0c850095-f0f9-480e-98a8-d6364edf2bbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "448960"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWrm0Dj_9svH"
      },
      "source": [
        "Balanced Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CuHYJSftvI9",
        "outputId": "aada9d59-0e79-4733-b903-1f674ef29432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected 50000 pairs: 25000 from label 1 and 25000 from label 0.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Number of samples to select for each label\n",
        "samples_per_label = 25000\n",
        "\n",
        "# Separate pairs by label\n",
        "label_1_pairs = [pair for pair in pairs if pair[2] == 1]  # Label 1\n",
        "label_0_pairs = [pair for pair in pairs if pair[2] == 0]  # Label 0\n",
        "\n",
        "# Randomly sample `samples_per_label` pairs from each group\n",
        "sampled_label_1_pairs = random.sample(label_1_pairs, min(samples_per_label, len(label_1_pairs)))\n",
        "sampled_label_0_pairs = random.sample(label_0_pairs, min(samples_per_label, len(label_0_pairs)))\n",
        "\n",
        "# Combine the samples\n",
        "balanced_pairs = sampled_label_1_pairs + sampled_label_0_pairs\n",
        "\n",
        "# Shuffle the combined pairs\n",
        "random.shuffle(balanced_pairs)\n",
        "\n",
        "print(f\"Selected {len(balanced_pairs)} pairs: {len(sampled_label_1_pairs)} from label 1 and {len(sampled_label_0_pairs)} from label 0.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2cEi_IU9Y5Z"
      },
      "source": [
        "Pair Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkF5QSHRuWXA"
      },
      "outputs": [],
      "source": [
        "# Library\n",
        "\n",
        "def process_pair(pair):\n",
        "    audio1_path, audio2_path, label = pair\n",
        "    features_audio1 = extract_features(audio1_path)\n",
        "    features_audio2 = extract_features(audio2_path)\n",
        "    combined_features = np.concatenate((features_audio1, features_audio2))\n",
        "    return combined_features, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRdFiW-JZcCS",
        "outputId": "e25c8823-17ae-47d6-c978-b46f191bea4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|███████████████████████████████▏                                            | 20548/50000 [49:07<52:34,  9.34it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Function to process pairs and save results\n",
        "def compute_and_save_results(pairs, save_path=\"results.pkl\"):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = list(tqdm(executor.map(process_pair, pairs), total=len(pairs)))\n",
        "\n",
        "    # Save results to file\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(results, f)\n",
        "    print(f\"Results saved to {save_path}\")\n",
        "\n",
        "# Compute and save results\n",
        "compute_and_save_results(balanced_pairs, save_path=\"results.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZjBcQXf9Xnn"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTah6vOoZgXG",
        "outputId": "ba3b5f98-2f0e-4245-fe70-25c69b616df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results loaded from results.pkl\n"
          ]
        }
      ],
      "source": [
        "def load_results(file_path=\"results.pkl\"):\n",
        "    \"\"\"\n",
        "    Loads precomputed results from a file.\n",
        "    :param file_path: Path to the saved results.\n",
        "    :return: List of feature pairs and labels.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        results = pickle.load(f)\n",
        "    print(f\"Results loaded from {file_path}\")\n",
        "    return results\n",
        "\n",
        "# Load results\n",
        "results = load_results(\"results.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdD2Nu0uC_Gr"
      },
      "outputs": [],
      "source": [
        "# SPLIT\n",
        "# Separate features and labels\n",
        "X, y = zip(*results)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVbtlqjuKQ21",
        "outputId": "acede29e-aa28-4608-aece-77d9f3a5e91a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5SLNxoH_0we"
      },
      "source": [
        "SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKzRvPhlyTyx",
        "outputId": "4003ad11-2ff5-49ad-f704-3a821b5b8a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 66.10%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.63      0.65       983\n",
            "           1       0.66      0.69      0.67      1017\n",
            "\n",
            "    accuracy                           0.66      2000\n",
            "   macro avg       0.66      0.66      0.66      2000\n",
            "weighted avg       0.66      0.66      0.66      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train a classical ML model (SVM in this case)\n",
        "model = SVC(kernel='linear', probability=True)  # Use 'rbf' kernel for non-linear decision boundaries\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxIFDi83C6zT"
      },
      "source": [
        "GMM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCnb6acFC7w-"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Train a GMM model\n",
        "model = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n",
        "model.fit(X_train)\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # Probability of being in the \"1\" class\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcc0XbhEBZGX"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "def save_model(fname, model):\n",
        "    \"\"\"Save the model to a file using pickle.\"\"\"\n",
        "    print(f\"Saving the model to {fname}...\")\n",
        "\n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    print(f\"Model saved successfully to {fname}.\")\n",
        "\n",
        "save_model(\"SVModel\",model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkc1PlwJJXxt",
        "outputId": "4ae35d49-3a10-4bf0-98bb-bf0ee94416dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the result.pkl file containing extracted pair features and labels\n",
        "with open('results.pkl', 'rb') as file:\n",
        "    results = pickle.load(file)\n",
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzxzRe55JXxu",
        "outputId": "dc8b9a33-5951-46dc-e1d6-bb505e5b1dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of SVM model with K-means clustering: 99.92%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Separate features and labels\n",
        "X, y = zip(*results)  # Extract features and labels\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Standardize the features for better performance in SVM and KMeans\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-means clustering to the features\n",
        "n_clusters = 2  # We are assuming binary classification (same or different speakers)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_scaled)  # Get the cluster labels as target for SVM\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, kmeans_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier using the cluster labels\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the SVM model\n",
        "y_pred = svm_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of SVM model with K-means clustering: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the trained SVM model and KMeans model for later use\n",
        "with open('svm_model_with_kmeans.pkl', 'wb') as file:\n",
        "    pickle.dump(svm_model, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DFwLC-MJXxu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}